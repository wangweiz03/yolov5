## 第1次上机作业：YOLOv5模型测试

### 姓名：<u>王炜致</u> 班级：<u>无22</u> 学号：<u>2022010542</u>

#### Task1. 查看 Ultralytics 提供的官方教程，使用 YOLOv5s 完成推理，计算<u>检测性能</u>（至少汇报`mAP@50` 和 `mAP@50:5:95`）

首先考虑`annotations.json`，它包含`'images','annotations','categories'`三种信息 ，其中`'images'`主要保存了所有 **图片（共367张）** 的`id`（具有唯一性）以及尺寸等特征，`'annotations'`主要保存了所有**Ground Truth框**的图片归属（依据`id`）、类别、位置及尺寸等特征，`'categories'`保存了模型认识的所有**类别（共80类）**.

为了调用YOLOv5自带的指标测量服务，在根目录构造自定义测试集所需要的配置文件：

```
custom_dataset/
├── images/
│   ├── test/  # 测试集图片
│   │   ├── file_name[0].jpg
│   │   └── ...
├── labels/
│   ├── test/  # 测试集标签
│   │   ├── file_name[0].txt
│   │   └── ...
└── dataset.yaml  # 数据集配置文件
```

其中`images`文件夹即为课程任务包提供的测试集图片文件夹；`labels`文件夹需要**自行构造**，其中每个`txt`文件涵盖了每张图片（从`annotations.json`中提取，与`images`中图片按文件名一一对应）中所有Ground Truth框的类别及位置信息，格式如下：

```
label_id(0~79) x_center y_center width height # GT BBOX 0
label_id(0~79) x_center y_center width height # GT BBOX 1
...
```

其中4个位置信息参数为**对图片长宽归一化**的结果.此外还需从`annotations.json`提取`'categories'`信息（或者照抄`./data/coco128.yaml`等配置文件）构造配置文件`dataset.yaml`，格式如下：

```
train: ../custom_dataset/images/train # 训练集，可以忽略
val: ../custom_dataset/images/val # 验证集，可以忽略
test: ../custom_dataset/images  # 测试集路径

# 类别数量
nc: len(annotations_json['categories'])

# 类别名称，需要'categories'信息或照抄./data/coco128.yaml等
names:
  'id': 'name'
  ...
```

构造任务由`json2list.py`脚本完成（见`./custom_dataset`），其中构造labels的代码如下，主要思路是先遍历`annotations.json`中的`'images'`，建立基于id的字典骨架；然后遍历`'annotations'`，将每个框的信息根据其id归属到对应的字典条目下.

```python
def mk_labels(path='./annotations.json'):
    with open(path,'r') as f:
        data = json.load(f)

    cat = data['categories']
    cat_dict = {obj['id']: idx for idx,obj in enumerate(cat)}

    images = data['images']
    ann = data['annotations']

    # 先遍历'images'
    txts_info_dict = {id['_image_id']: [(id['width'],id['height'])] for id in images}

    # 然后遍历'annotations'
    for an in ann: 
        x,y,w,h = an['bbox']
        idd = an['category_id']
        X = (x+w/2)/txts_info_dict[an['_image_id']][0][0]
        Y = (y+h/2)/txts_info_dict[an['_image_id']][0][1]
        W = w/txts_info_dict[an['_image_id']][0][0]
        H = h/txts_info_dict[an['_image_id']][0][1]
        txts_info_dict[an['_image_id']].append([cat_dict[idd],X,Y,W,H])

    for an in txts_info_dict:
        with open('./labels/'+an+'.txt','w') as f:
            txt_info_cur = txts_info_dict[an]
            for idx in range(1,len(txt_info_cur)):
                f.write(f"{txt_info_cur[idx][0]:.0f} 
                {txt_info_cur[idx][1]:.6f} 
                {txt_info_cur[idx][2]:.6f} 
                {txt_info_cur[idx][3]:.6f} 
                {txt_info_cur[idx][4]:.6f}\n")
```

随后可以在根目录下运行指令，进行测试：

```
python val.py --data ./custom_dataset/dataset.yaml --task test
```

结果为：
```
Class     Images  Instances     P          R         mAP50    mAP50-95
all        367        599      0.523      0.474      0.374      0.268
Speed: 5.1ms pre-process, 327.3ms inference, 
3.4ms NMS per image at shape (32, 3, 640, 640)
```

**其中`mAP50`即`mAP@50`，它的值为`0.374`，表达的是模型在`IoU`阈值为`0.50`时所有类别的平均精度（Average Precisions）对类别再取平均（mean）的结果；`mAP50-95`即`mAP@50:5:95`，它的值为`0.268`，表达的是模型在`IoU`阈值从`0.50`到`0.95`范围内（步长为`0.05`）的平均精度（取了3次平均，分别为单类别内平均、所有类别平均、所有`IoU`平均）.** 可见阈值过高会导致精度下降，这与直觉相符.

---

#### Task2. 对 ground truth 标注和模型预测的结果进行<u>可视化</u>，找到典型失败案例并适当分析原因

可视化结果如下（取部分样例，左图为测试集GT标注情况，右图为模型预测结果）.

**标注情况1、预测结果1：**
![alt text](val_batch0_labels.jpg){:height="48%" width="48%"} ![alt text](val_batch0_pred.jpg){:height="48%" width="48%"}

**标注情况2、预测结果2：**
![alt text](val_batch1_labels.jpg){:height="48%" width="48%"} ![alt text](val_batch1_pred.jpg){:height="48%" width="48%"}

**标注情况3、预测结果3：**
![alt text](val_batch2_labels.jpg){:height="48%" width="48%"} ![alt text](val_batch2_pred.jpg){:height="48%" width="48%"}

分析结果——容易发现，由于模型能够识别的类别有限（80类），而测试集图片包含的物体种类极为繁多，因此（为适应模型有限的认知水平）多数测试集图片并未附带给出GT框.然而，正是在这些未标出GT框的情形下，模型的预测结果出现了非常严重的**虚警情况**，即强行把并非某种类的物体识别成了该种类，并框出，可以说没有做到“知之为知之，不知为不知”. 例如模型不认识“硬币”，但把图中的硬币指认为形状相似的“clock”；不认识“狮子”，但把狮子指认为“dog”.

但另一方面，对于**模型已知的、测试集有GT框标注的正例物体，模型的响应及判断准确性很高**.综上，模型的召回率（Recall）很高（漏检的正例框很少），但精确率（Precision）很低（错检出的伪正例框过多）.

#### Task3. 调整可能调整的配置，包括但不限于更换模型（n/m/l/x etc.）或使用 Test Time Augmentation(TTA) 等，尝试提高性能，展示<u>探索过程</u>

上述测试过程默认使用了`yolov5s.pt`模型，现在考虑**更换模型**.利用下述指令将`n/m/l/x`模型下载到项目根目录下：

```
python -c "from utils.downloads import attempt_download; attempt_download('yolov5n.pt')"  # nano
python -c "from utils.downloads import attempt_download; attempt_download('yolov5m.pt')"  # medium
python -c "from utils.downloads import attempt_download; attempt_download('yolov5l.pt')"  # large
python -c "from utils.downloads import attempt_download; attempt_download('yolov5x.pt')"  # xlarge
```

![alt text](image.png)

随后对相同的自定义数据集及4个不同规模的模型，运行指令：

```
python val.py --weights yolov5n.pt --data ./custom_dataset/dataset.yaml --task test
...
```

由于设备配置(CPU)限制，运行`m,l,x`模型时遇到了算力不足而无法完成测试的问题，**考虑减小batch大小（参数设置`--batch 4`）**，并**重新**测量`n,s`两个模型.

得到测试结果，完整结果保存于`./runs/val`下，基本指标列表如下`Class:all, Images:367, Instances:599, shape:(4, 3, 640, 640)`：
| **模型**   | `P` | `R` | `mAP50` | `mAP50-95` | `T/img`
|-----------|------------|------------|------------|------------|------------|
| **yolov5n**  | `0.435` | `0.318` | `0.296` | `0.207` | **104.1ms** |
| **yolov5s** | **0.530** | `0.473` | `0.377` | `0.271` | `204.8ms` |
| **yolov5m** | `0.311` | **0.606** | `0.405` | `0.314` | `413.7ms` |
| **yolov5l** | `0.409` | `0.580` | `0.440` | `0.356` | `743.4ms` |
| **yolov5x** | `0.472` | `0.518` | **0.462** | **0.374** | `1116.0ms`

考虑`mAP50,mAP50-95`及时间指标，可见随着模型体量增大，`mAP`指标的提升比较显著，从`0.296`进展到`0.462`,可见**更换体量更大的模型有助于提高任务性能**；但相应的推理时间开销也增大，从轻量模型的`104.1ms`增长到`1116.0ms`.

#### Task4. 提出<u>改进建议</u>，可以参考 YOLO 系列模型后续版本是为了解决哪些问题做出哪些改进的







